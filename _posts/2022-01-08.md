---
layout: post
title: ""
date:   2022-01-08 08:00:00 +0200
image: /assets/aem/hack.jpg
seo_description: How Amateur Radio Community Is Doing After Almost 100 Years?
categories: working
---

![My own FT 897 radio rig](/assets/aem/hack.jpg)

In this blog post I am going to demonstrate how easy is to gather an information of which companies are using Adobe Experience Manager on which websites, for further black-hat or just business operations. Like

<!-- more -->

## Adobe Experience Manager Recognized Easily

AEM is a very specific CMS that uses very strong path organization - including how assets, pages and scripts are stored. In very default configuration, all pages are stored under `/content...`, scripts `/etc.clientlibs` (`/etc/designs...` for older versions) and assets `/content/dam`. Leveraging this fact, I am considering to find out which websites are using this CMS.

## Value of Information

How that knowledge can be used? Organisations using AEM are usually visible, with high credibility, and, very often, making very lucrative contracts with vendors handling AEM instances. Instances are not handling the organisation's core activity but are informative webistes, but even then distruptions, like increasing the page load time, may be an important part of a larger operation against the target. If the attacker knows the fact AEM is used, it's certainly easier to prepare a set of attacks for the website. If this is extremely easy to find out, attacker can pick the target based on that fact.

From another perspective, AEM vendors are fighting for each perimieter of AEM world - which is very natural in such market. The fact one can easily find out which large-size corporations use AEM may be a cause of taking much more effective advertisement/marketing actions.

To sum up, posesing this information is certainly good for black-hat AEM hackers and also vendors.

## Existing Solution

[AEM Hacker](https://github.com/0ang3el/aem-hacker) is a project for discovering AEM's vulerabilities automatically, based on the website name. It is very often used by developers and testers to verify website's security. There is a file which may be used to discovering the AEM based on specially prepared HTTP requests.

In this article, however, I am going to perform such discovery based on reading the regular response, so that the attacker cannot be easily discovered in this stage, making very small amount of absolutely legitimate requests.

I personally use AEM Hacker software for checking the state of the existing website.

## Approach

The hypothesis is AEM websites can be discovered based on the paths to assets, scripts or websites. To make this working globally, the test have to be performed over random websites, check their HTML response and mark ones that are probably served from AEM.

Therefore, the highest-level alghorithm is as follows:
* read the website
* check if it has any links to /content or /content/dam or /etc.clientlibs
* if the above is true, save website's host name
* move to another page (jump to the beginning if possible)
* return list of saved host names - here's the list of AEM instances

Crawling pages performs as graph exploration, so that links for the websites are not known upfront but discovered during crawling. To crawl over the website I have decided to use [Scrapy](https://scrapy.org/), which is similar tech stack as AEM Hacker (Python executable).

## Testing The Hypothesis

This simple program checks whether the website is AEM or not. For my testing list of websites, the accuracy is 100%.

```
def check(base_url, debug, proxy):

    response = http_request(base_url)
    result = re.findall('[\w]*[ ]*=[ ]*\"(?:/content/dam/|/etc.clientlibs/)[^\"]+\"', response.text)
    if result:
        return result
    else:
        return []
```

This example shows that the check is based only on regular expression that seeks the link of attribute in the entire HTML.

## Implementing The Actual Program

Implementation of the program relies on Scrapy's architecture - that it requires to running the codebase with Spider's derivative.

```
import scrapy
import re

MAX_DEPTH = 3

class QuotesSpider(scrapy.Spider):
    name = 'quotes'
    start_urls = [
        'https://www.economist.com/',
    ]

    hosts = {}

    def is_in_hosts(self, url, origin):
        if re.search('^/', url):
            host = origin
            print("Host: " + host)
        elif re.search('^(http|https)?:?//', url):
            host = re.search('//([^/]+)/?', url).group(1)
        else:
            host = "None"
        if host not in self.hosts:
            return False

        return self.hosts[host] >= MAX_DEPTH

    def parse(self, response):

        if not re.search('text/html', response.headers['Content-Type'].decode('utf-8')):
            return

        for quote in response.xpath('//*[@*[contains(., \'/etc.clientlibs/\')]]'):
            yield {
                'src': quote.get(),
                'page': response.url,
            }
        for resource in response.xpath('//*[@*[starts-with(., \'/content/dam/\')]]'):
            yield {
                'src': resource.get(),
                'page': response.url,
            }

        host = re.search('//([a-zA-Z0-9.]+)/', response.url).group(1)

        if host not in self.hosts:
            self.hosts[host] = 0

        if host in self.hosts and self.hosts[host] < MAX_DEPTH:
            self.hosts[host] = self.hosts[host] + 1

            next_pages = response.xpath('//a/@href').getall()
            if next_pages is not None:
                for next_page in next_pages:
                    if not (re.search('^(tel|mailto):', next_page) or self.is_in_hosts(next_page, host)):
                        yield response.follow(next_page, self.parse)
```

This code has to be executed using Scrapy's environment by:

```
scrapy runspider .\find_pages.py -o result.jl
```

Once executed, `result.jl` contains new-line separated JSON objects with the results of crawling. Example:

```
{"src": "<script type=\"text/javascript\" src=\"/etc.clientlibs/digicert/clientlibs/clientlib-base.js\"></script>", "page": "https://www.geotrust.com"}
```

I haven't resigned to validate those entries, hence I created an additional script that combines these into list of host names based on a single map to reduce duplicates:

```
import json
import re

file = open("./result.jl")

def host(url):
    return re.search('^(?:https?)?:?//([^/]+)', url).group(1)

result = {}

for line in file:
    obj = json.loads(line)
    result[host(obj['page'])] = 1

for domain in result:
    print(domain)
```

## Results

After running the script for 30 minutes I have discovered the list of some websites:

* business.adobe.com
* brand.linkedin.com
* legal.twitter.com
* safety.linkedin.com
* about.linkedin.com
* www.admin.ch
* www.edoeb.admin.ch
* www.uvek.admin.ch
* www.rumba.admin.ch
* www.vorbild-energie-klima.admin.ch
* www.naturgefahren.ch
* www.bfe.admin.ch
* www.natural-hazards.ch
* www.pericoli-naturali.ch
* www.privels-natira.ch
* www.stelle.admin.ch
* www.ejpd.admin.ch
* www.ekm.admin.ch
* www.sem.admin.ch
* www.metas.ch
* www.eschk.admin.ch
* www.esbk.admin.ch
* www.nkvf.admin.ch
* www.isc-ejpd.admin.ch
* www.bj.admin.ch
* www.fedpol.admin.ch
* www.ncsc.admin.ch
* www.wbf.admin.ch
* www.efd.admin.ch
* www.vbs.admin.ch
* www.isceco.admin.ch
* www.bk.admin.ch
* www.weko.admin.ch
* www.eda.admin.ch
* www.seco.admin.ch
* www.adobe.com
* www.unilevernotices.com
* www.synopsys.com
* www.bsimm.com
* us.aicpa.org
* www.salesforce.com
* business.amazon.it
* www.geotrust.com
* www.digicert.com
* knowledge.digicert.com

## Potential Enhancements

The script is very simple and its 
